{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFORMER\n",
    "- [REFORMER: The Efficient Transformer](https://arxiv.org/pdf/2001.04451.pdf)\n",
    "- [Github of patrickvonplaten for Reformer](https://github.com/patrickvonplaten/notebooks)\n",
    "- [Reformer in Huggingface](https://huggingface.co/transformers/model_doc/reformer.html)\n",
    "\n",
    "#### 메모리를 적게 사용하기 위해 Reformer가 도입한 방법들\n",
    "1. Axial Positional Encoding\n",
    "2. LSH Attention (Locality-Sensitive Hashing Attention)\n",
    "3. Reversible Residual Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Axial Positional Encoding\n",
    "- 긴 input sequence가 주어지는 경우 standard positional encoding은 너무 많은 메모리를 사용함 \n",
    "    - $d_h$: hidden size, ```config.hidden_size```\n",
    "    - $n_{max}$: max position embeddings, ```config.max_position_embeddings``` *(defaults to 4096)*\n",
    "    - Standard Positional Encoding: $n_{max} \\times d_h$\n",
    "    - Axial Positional Encoding: $n_\\text{max}^1 \\times d_h^1 + n_\\text{max}^2 \\times d_h^2$\n",
    "        - $n_\\text{max}^1 \\times n_\\text{max}^2 = n_\\text{max}$\n",
    "            - ```config.axial_pos_shape```: a tuple $(n_\\text{max}^1, n_\\text{max}^2)$\n",
    "        - $d_h^1 + d_h^2 = d_h$\n",
    "            - ```config.axial_pos_embds_dim```: a tuple $(d_h^1, d_h^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "$d_h = 4$, $n_{max} = 49$\n",
    "#### 1. Standard Positional Encoding\n",
    "![img](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/positional_encodings_default.png)\n",
    "<br>\n",
    "- 길이가 4인 49개의 벡터\n",
    "- $4 \\times 49$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Axial Positional Encoding\n",
    "- $n_\\text{max}^1 =7$,  $n_\\text{max}^2 = 7$\n",
    "- $d_h^1 = 1$, $d_h^2 = 3$\n",
    "<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/3d_positional_encoding.png\"  width=\"400\"/>\n",
    "<br>\n",
    "- 길이가 4인 49개의 벡터가 7개씩 7줄로 배열되어 있음 $(n_\\text{max}^1 \\times n_\\text{max}^2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text2](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/3d_positional_encoding_cut.png)\n",
    "<br>\n",
    "- 첫 번째 열의 7개 벡터만으로 나머지 벡터를 모두 나타내고자 함\n",
    "- $e_{up} = d_{h}^2 = 3$\n",
    "- $e_{down} = d_{h}^1 = 1$\n",
    "<br><br>\n",
    "- 7개 벡터의 윗부분(길이 3)은 column으로 확장함\n",
    "    - 첫번째 열의 벡터들($e_1$ ~ $e_7$) 윗부분 3만큼은 모두 첫번째 열의 첫 번째 벡터($e_1$)의 윗부분으로 채워짐 \n",
    "    - 두번째 열의 벡터들($e_8$ ~ $e_{14}$) 윗부분 3만큼은 모두 첫번째 열의 두 번째 벡터($e_2$)의 윗부분으로 채워짐\n",
    "    - ...\n",
    "    - 일곱번째 열의 벡터들($e_{43}$ ~ $e_{49}$) 윗부분 3만큼은 모두 첫번째 열의 일곱번째 벡터($e_7$)의 윗부분으로 채워짐\n",
    "<br><br>\n",
    "- 7개 벡터의 아래부분(길이 1)은 row로 확장함\n",
    "    - 첫번째 행의 벡터들($e_1, e_8, ... , e_{43}$) 아랫부분 1만큼은 모두 첫번째 열의 첫 번째 벡터($e_1$)의 아랫부분으로 채워짐\n",
    "    - 두번째 행의 벡터들($e_2, e_9, ... , e_{44}$) 아랫부분 1만큼은 모두 첫번째 열의 두 번째 벡터($e_2$)의 아랫부분으로 채워짐  \n",
    "    - ... \n",
    "    - 일곱번째 행의 벡터들($e_7, e_{14}, ... , e_{49}$) 아랫부분 1만큼은 모두 첫번째 열의 일곱 번째 벡터($e_7$)의 아랫부분으로 채워짐  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/axial_pos_encoding.png\"  width=\"500\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "- $e'_1, e'_2, ... e'_{49}$ 중에 같은 벡터가 하나도 없음\n",
    "\n",
    "- $n_\\text{max}^1 \\times d_h^1 + n_\\text{max}^2 \\times d_h^2$만으로 positional encoding 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSH Attention\n",
    "- 기존 attention 계산에서 메모리 많이 사용하는 부분: $QK^T$\n",
    "    - Q, K, T shape 모두 *\\[batch size, length, $d_{model}$\\]* 이라고 하면, $QK^T$의 shape은 *\\[batch size, length, length\\]*\n",
    "        - ```config.attention_head_size```: $d_{model}$\n",
    "<img src=\"https://github.com/yeounyi/reformer_summary/blob/main/img/1.png?raw=true\" width=\"300\"/>\n",
    "<br>\n",
    "\n",
    "#### 2.1. 하나의 query마다 attention 계산\n",
    "-  attention을 matrix로 한 번에 계산하지 않고, 하나의 query 마다 계산한다면, $qK^T$의 shape은 *\\[batch size, 1, length\\]* \n",
    "    - only use memory proprtional to *length* \n",
    "<img src=\"https://github.com/yeounyi/reformer_summary/blob/main/img/2.png?raw=true\" width=\"200\"/>\n",
    "<br>\n",
    "\n",
    "#### 2.2. Q = K\n",
    "- shared-QK Transformer\n",
    "- 성능엔 큰 영향 없음\n",
    "- first token 제외 자기 자신에게 attend하는 것 금지 \n",
    "\t-  Q = K 니까 항상 자기 자신과의 dot-product가 가장 값이 큼\n",
    "    - cf. original Transformer는 자기 자신도 attend 가능 \n",
    "\n",
    "#### 2.3. Hashing Attention\n",
    "- $softmax(QK^T)$에서 어차피 가장 큰 값만 지배적인 역할을 하고 나머지는 큰 영향 주지 않음 \n",
    "- $Q=K$ 이므로 각 $q_i$에 대해 가장 가까운 $key$들을 모두 찾아야 하는 것이 아니라, $q$끼리 가까운 것만 찾으면 됨 \n",
    "- 유사한 $q$끼리 하나의 cluster로 모아 $m$개의 cluster를 만들 수 있음\n",
    "<img src=\"https://github.com/yeounyi/reformer_summary/blob/main/img/4.png?raw=true\" width=\"300\"/>\n",
    "<br>\n",
    "- 그러면 같은 cluster 내에서만 softmax를 계산해도 됨. 유사하지 않은 모든 K(=Q)까지 모두 포함해서 계산할 필요 없음\n",
    "<img src=\"https://github.com/yeounyi/reformer_summary/blob/main/img/5.png?raw=true\" width=\"300\"/>\n",
    "<br>\n",
    "\n",
    "#### LSH (Locality Sensitive Hashing)\n",
    "- 유사한 벡터를 찾는 빠른 방법\n",
    "- x → h(x)에 대응, 가까운 x끼리는 높은 확률로 같은 hash값을 갖게 됨 \n",
    "<img src=\"https://github.com/yeounyi/reformer_summary/blob/main/img/7.png?raw=true\"/>\n",
    "<br>\n",
    "- 위의 x,y는 random rotation 했을 때 다른 구역에 위치하는 경우가 많아서 높은 확률로 다른 hash 값을 갖게 됨 \n",
    "- 아래 x,y는 random rotation 해도 계속 같은 구역에 위치해서 높은 확률로 같은 hash 값을 갖게 됨 \n",
    "- hash 여러 번 해서 정확도 높임 \n",
    "    - ```config.num_hashes```: 몇 번의 hash를 할 지 \n",
    "\n",
    "<img src=\"https://github.com/yeounyi/reformer_summary/blob/main/img/9.png?raw=true\" width=600/>\n",
    "<br>\n",
    "\n",
    "- 같은 hash 값을 갖는 query끼리 하나의 bucket으로 배정하고 각 bucket끼리 모일 수 있도록 sorting\n",
    "    - ```config.num_buckets```: 몇 개의 bucket으로 나눌지 \n",
    "- sorting된 sequence를 chunk로 나누기 \n",
    "    - chunk로 나누지 않으면 bucket마다 크기가 달라서 batch 처리 어려움 \n",
    "    - ```config.lsh_attn_chunk_length```: chunk의 길이\n",
    "- 자신이 속해 있는 chunk + 이전 chunk까지만 attend \n",
    "    - chunking 때문에 하나의 bucket이 다른 chunk로 쪼개질 수 있으니 이전 bucket까지 attend할 수 있어야 함\n",
    "    - ```config.lsh_num_chunks_before```: 이전의 몇 개의 chunk까지 attend할 수 있게 할지 *(defaults to 1)*\n",
    "    - ```config.lsh_num_chunks_after```: 이후의 몇 개의 chunk까지 attend할 수 있게 할지 *(defaults to 0)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reversible Residual Layer\n",
    "- RevNet의 아이디어를 Transformer에 적용함 \n",
    "- Reversible Layer + Chunking 으로 메모리 사용량이 layer 개수와 independent 해짐 \n",
    "\n",
    "\n",
    "#### 3.1. RevNet to Transformer\n",
    "\n",
    "#### RevNet\n",
    "- model parameter만 사용하면서 특정 layer의 activation을 그 다음 layer의 activation으로 복원할 수 있음\n",
    "- back propagation을 위해 모든 activation을 저장해놓을 필요가 없음\n",
    "- reversible layer는 input, ouput을 pair로 받음  \n",
    "    - input: ($x_1, x_2$)\n",
    "    - output: ($y_1, y_2$)\n",
    "    \n",
    "<img src=\"https://github.com/yeounyi/reformer_summary/blob/main/img/13.png?raw=true\" width=500/>\n",
    "<br>\n",
    "\n",
    "- substracting을 통해 output ($y_1, y_2$)만 갖고 input ($x_1, x_2$)을 복원할 수 있음 \n",
    "\n",
    "<img src=\"https://github.com/yeounyi/reformer_summary/blob/main/img/18.png?raw=true\" width=500/>\n",
    "<br>\n",
    "\n",
    "#### Transformer\n",
    "- RevNet의 F를 attention layer, G를 feed-forward layer로 대체 \n",
    "- output of the last reversible transformer layer 만 저장하면 back propagation 가능 \n",
    "<br>\n",
    "<img src=\"https://github.com/yeounyi/reformer_summary/blob/main/img/14.png?raw=true\" width=500/>\n",
    "<br>\n",
    "\n",
    "#### 3.2. Chunking Feed Forward Layer\n",
    "- Attention 이후 2개의 Feed Forward Layer: $\\mathbf{Y}_{\\text{out}} = \\text{Linear}_{\\text{out}}(\\mathbf{Y}_\\text{int}) = \n",
    "\\text{Linear}_{\\text{out}}(\\text{Linear}_{\\text{int}}(\\mathbf{\\overline{Z}}))$\n",
    "    - 여기서 $i$번째 output인 $y_{out,i}$는 $i$번째 input에만 영향 받고 다른 위치에 있는 input에는 영향 받지 않음\n",
    "    - 따라서 chunking 가능 \n",
    "<br><br>  \n",
    "- $Y_{int}$의 dimension이 $Y_{out}$의 dimension보다 큼. 더 많은 memory 필요\n",
    "    - ```config.feed_forward_size```: $Y_{int}$의 output dimension\n",
    "    - ```config.hidden_size```: $Y_{out}$의 output dimension\n",
    "<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/feed_forward_matrix.png\" width=500/>\n",
    "<br>\n",
    "\n",
    "- chunking하여 계산 후 concat 하면 커다란 Y_int matrix 전체를 다 저장해놓을 필요없어 memory 절약 가능\n",
    "    - 그러나 시간은 좀 더 걸림 \n",
    "    - memory와 time의 trade off \n",
    "<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/chunked_feed_forward.png\" width=500/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
